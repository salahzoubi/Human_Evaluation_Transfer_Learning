{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7607cbd-e0ec-4a4b-8f62-d50f02692d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import *\n",
    "from dataset import TLDataset\n",
    "from transformers import T5ForConditionalGeneration,AutoTokenizer, AutoConfig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from finetuner import T5FineTuner\n",
    "import torch \n",
    "from evaluate import *\n",
    "from multiprocessing import cpu_count\n",
    "import argparse\n",
    "output_path_dir = \"/home/salzubi_umass_edu/CKPTS/OAI_TRG_MODEL_CKPT/\"\n",
    "train_bsz = 4\n",
    "lr = 1e-3\n",
    "train_fraction = 1\n",
    "model_name = \"google/flan-t5-base\"\n",
    "file_name = f\"OAI_trg_MODEL_task_Adam_lr:{lr}_bsz:{train_bsz}_frac:{train_fraction}_{model_name}\"\n",
    "num_gpus = torch.cuda.device_count()\n",
    "num_cpus = cpu_count()\n",
    "\n",
    "args_dict = dict(\n",
    "    # file_name = file_name,\n",
    "    # output_dir=output_path_dir, # path to save the checkpoints\n",
    "    model_name_or_path=model_name,\n",
    "    tokenizer_name_or_path=model_name,\n",
    "    max_seq_length=512,\n",
    "    train_fraction= train_fraction,\n",
    "    learning_rate=lr,\n",
    "    weight_decay=0.0,\n",
    "    scheduler_factor = .1,\n",
    "    use_gpu = True if num_gpus > 0 else False,\n",
    "    train_batch_size=train_bsz,\n",
    "    eval_batch_size=32,\n",
    "    num_train_steps=10000000,\n",
    "    es_patience = 4,\n",
    "    val_check_interval = .25,\n",
    "    dropout = .2,\n",
    "    n_gpu=num_gpus,\n",
    "    cpu_per_device=num_cpus,\n",
    "    task = \"classification\",\n",
    ")\n",
    "\n",
    "args = argparse.Namespace(**args_dict)\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "494c467c-beb1-47d8-af20-0fc562a4614e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-03-23 14:25:20.159490: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-23 14:25:23.360971: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /modules/apps/julia/1.7.2/lib:/modules/apps/cuda/11.3.1/lib64\n",
      "2023-03-23 14:25:23.363117: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /modules/apps/julia/1.7.2/lib:/modules/apps/cuda/11.3.1/lib64\n",
      "2023-03-23 14:25:23.363141: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from evaluate import *\n",
    "from sklearn.metrics import accuracy_score, r2_score, mean_squared_error\n",
    "import glob\n",
    "trg_task = [11]\n",
    "test_split = .2\n",
    "fixed_path = \"../experiments/\"\n",
    "# model_path = \"SRC_task_[3]___TRG_task___[8]\"\n",
    "model_path = \"SRC_task_[50, 40, 32, 31, 51, 39, 52, 53, 55]___TRG_task___[35]\"\n",
    "\n",
    "scores = []\n",
    "\n",
    "for i in range(1,3):\n",
    "    random_seed = i\n",
    "    # ckpt_path = f\"{fixed_path}{model_path}/trial_{i}/{model_path}___trial_{i}-v1.ckpt\"\n",
    "    eval_path = f\"{fixed_path}{model_path}/trial_{i}/{model_path}___trial_{i}_eval.csv\"\n",
    "    pred_df = pd.read_csv(eval_path)\n",
    "    # train, val, test = generate_normal_splits(data_path = \"/work/salzubi_umass_edu/human_eval_datasets/master_dataset.csv\", random_state=random_seed,  trg_task_ids= trg_task,  split_size=test_split, source_tuning=False)\n",
    "    # test_model = load_model(args, ckpt_path)\n",
    "    # test_data = TLDataset(test, tokenizer)\n",
    "    # test_loader = DataLoader(test_data, batch_size=args.eval_batch_size, num_workers=num_cpus) #, num_workers=num_cpus//num_gpus\n",
    "    # pred_df = evaluate_batch(test_loader, test_model, tokenizer)\n",
    "    scores += [accuracy_score(pred_df.predicted, pred_df.ground_truth)]\n",
    "    # scores += [mean_squared_error(pred_df.predicted, pred_df.ground_truth)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1ae470d-4b6a-47b6-a0f2-ed5eea0f4b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7574468085106383, 0.7170212765957447]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de56af7f-ad65-46f7-aed4-c163ee58132b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7372340425531915\n",
      "\n",
      "0.02021276595744681\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.mean(scores))\n",
    "print()\n",
    "print(np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a17d029-7ac4-49dd-850c-df55c1a9f3ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71346efb-7397-4419-acac-b94bce321c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18f63bf-88fc-46ab-9413-debd7d940f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import *\n",
    "\n",
    "train, val = generate_normal_splits(data_path = \"/work/salzubi_umass_edu/human_eval_datasets/master_dataset.csv\", random_state=1,\n",
    "                                    trg_task_ids=[24,25,26],\n",
    "                                    train_fraction=1, source_tuning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad074ea5-9248-4a9e-a235-6f38259c50d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import *\n",
    "from evaluate import *\n",
    "from dataset import TLDataset\n",
    "from transformers import T5ForConditionalGeneration,AutoTokenizer, AutoConfig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import argparse\n",
    "import logging\n",
    "import torch\n",
    "from multiprocessing import cpu_count\n",
    "from finetuner import T5FineTuner\n",
    "from ray_lightning import RayStrategy\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from tqdm import tqdm\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "import ray\n",
    "import ast\n",
    "import os\n",
    "import gc\n",
    "\n",
    "\n",
    "output_path_dir = \"/home/salzubi_umass_edu/CKPTS/OAI_TRG_MODEL_CKPT/\"\n",
    "train_bsz = 4\n",
    "lr = 1e-3\n",
    "train_fraction = 1\n",
    "model_name = \"google/flan-t5-base\"\n",
    "file_name = f\"OAI_trg_MODEL_task_Adam_lr:{lr}_bsz:{train_bsz}_frac:{train_fraction}_{model_name}\"\n",
    "num_gpus = torch.cuda.device_count()\n",
    "num_cpus = cpu_count()\n",
    "\n",
    "args_dict = dict(\n",
    "    # file_name = file_name,\n",
    "    # output_dir=output_path_dir, # path to save the checkpoints\n",
    "    model_name_or_path=model_name,\n",
    "    tokenizer_name_or_path=model_name,\n",
    "    max_seq_length=512,\n",
    "    train_fraction= train_fraction,\n",
    "    lr=lr,\n",
    "    weight_decay=0.0,\n",
    "    scheduler_factor = .1,\n",
    "    use_gpu = True if num_gpus > 0 else False,\n",
    "    train_batch_size=train_bsz,\n",
    "    eval_batch_size=32,\n",
    "    num_train_steps=10000000,\n",
    "    es_patience = 4,\n",
    "    val_check_interval = .25,\n",
    "    dropout = .2,\n",
    "    n_gpu=num_gpus,\n",
    "    cpu_per_device=num_cpus,\n",
    "    task = \"classification\",\n",
    ")\n",
    "\n",
    "args = argparse.Namespace(**args_dict)\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82605627-56c8-44d9-afa9-cac02dd8c4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import *\n",
    "\n",
    "task_ids = [33,59,21]\n",
    "train, val, _ = generate_normal_splits(data_path = \"/work/salzubi_umass_edu/human_eval_datasets/master_dataset.csv\", random_state=1,\n",
    "                                       trg_task_ids=task_ids, val_split=.3, split_size=.3, train_fraction=1, source_tuning=True, example_threshold=50000)\n",
    "\n",
    "train_data = TLDataset(train, tokenizer)\n",
    "val_data = TLDataset(val, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=8, drop_last=True) # , num_workers=num_cpus//num_gpus, sampler = train_sampler\n",
    "val_loader = DataLoader(val_data, batch_size=8) #, num_workers=num_cpus//num_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a45f16a-e5dd-4667-bf2d-314d5e0db43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=1079980)\u001b[0m 2023-03-25 15:16:48.165972: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=1079980)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=1079980)\u001b[0m 2023-03-25 15:16:48.313074: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=1079980)\u001b[0m 2023-03-25 15:16:50.174410: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /modules/apps/julia/1.7.2/lib:/modules/apps/cuda/11.3.1/lib64\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=1079980)\u001b[0m 2023-03-25 15:16:50.174584: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /modules/apps/julia/1.7.2/lib:/modules/apps/cuda/11.3.1/lib64\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=1079980)\u001b[0m 2023-03-25 15:16:50.174605: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=1079980)\u001b[0m /work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/pytorch_lightning/utilities/warnings.py:53: LightningDeprecationWarning: pytorch_lightning.utilities.warnings.rank_zero_deprecation has been deprecated in v1.6 and will be removed in v1.8. Use the equivalent function from the pytorch_lightning.utilities.rank_zero module instead.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=1079980)\u001b[0m   new_rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=1079980)\u001b[0m /work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/pytorch_lightning/utilities/warnings.py:58: LightningDeprecationWarning: ParallelStrategy.torch_distributed_backend was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=1079980)\u001b[0m   return new_rank_zero_deprecation(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=1079980)\u001b[0m Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=1079981)\u001b[0m 2023-03-25 15:16:59.259525: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=1079981)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=1079981)\u001b[0m 2023-03-25 15:16:59.405992: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=1079981)\u001b[0m 2023-03-25 15:17:00.764086: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /modules/apps/julia/1.7.2/lib:/modules/apps/cuda/11.3.1/lib64\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=1079981)\u001b[0m 2023-03-25 15:17:00.764255: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /modules/apps/julia/1.7.2/lib:/modules/apps/cuda/11.3.1/lib64\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=1079981)\u001b[0m 2023-03-25 15:17:00.764275: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=1079981)\u001b[0m Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=1079981)\u001b[0m Missing logger folder: lightning_logs\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=1079980)\u001b[0m ----------------------------------------------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=1079980)\u001b[0m distributed_backend=nccl\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=1079980)\u001b[0m All distributed processes registered. Starting with 2 processes\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=1079980)\u001b[0m ----------------------------------------------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=1079980)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=1079980)\u001b[0m GPU available: True (cuda), used: True (Please ignore the previous info [GPU used: False]).\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=1079980)\u001b[0m Missing logger folder: lightning_logs\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=1079981)\u001b[0m LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=1079980)\u001b[0m /work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:611: UserWarning: Checkpoint directory  exists and is not empty.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=1079980)\u001b[0m   rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=1079980)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "ename": "RayTaskError(AttributeError)",
     "evalue": "\u001b[36mray::RayExecutor.execute()\u001b[39m (pid=1079981, ip=10.100.115.3, repr=<ray_lightning.launchers.utils.RayExecutor object at 0x7fe0a847e700>)\n  File \"/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/ray_lightning/launchers/utils.py\", line 52, in execute\n    return fn(*args, **kwargs)\n  File \"/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/ray_lightning/launchers/ray_launcher.py\", line 301, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 811, in _fit_impl\n    results = self._run(model, ckpt_path=self.ckpt_path)\n  File \"/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1217, in _run\n    self.strategy.setup(self)\n  File \"/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp_spawn.py\", line 141, in setup\n    self.configure_ddp()\n  File \"/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp_spawn.py\", line 193, in configure_ddp\n    self.setup_optimizers(self.lightning_module.trainer)\n  File \"/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py\", line 128, in setup_optimizers\n    self.optimizers, self.lr_scheduler_configs, self.optimizer_frequencies = _init_optimizers_and_lr_schedulers(\n  File \"/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\", line 180, in _init_optimizers_and_lr_schedulers\n    optim_conf = model.trainer._call_lightning_module_hook(\"configure_optimizers\", pl_module=model)\n  File \"/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1595, in _call_lightning_module_hook\n    output = fn(*args, **kwargs)\n  File \"/work/salzubi_umass_edu/T5_human_eval_finetune/finetuner.py\", line 58, in configure_optimizers\n    optimizer = torch.optim.AdamW(self.parameters(), lr=self.h_params.lr)\nAttributeError: 'Namespace' object has no attribute 'lr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(AttributeError)\u001b[0m              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m T5FineTuner(args)\n\u001b[1;32m      9\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(max_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25000\u001b[39m, strategy\u001b[38;5;241m=\u001b[39mstrategy, callbacks \u001b[38;5;241m=\u001b[39m [es, checkpoint_callback], val_check_interval\u001b[38;5;241m=\u001b[39mval_check_interval, logger\u001b[38;5;241m=\u001b[39mtb_logger, replace_sampler_ddp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;66;03m# strategy = strategy\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:770\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;124;03mRuns the full optimization routine.\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;124;03m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 770\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:721\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 721\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlauncher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    723\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/ray_lightning/launchers/ray_launcher.py:58\u001b[0m, in \u001b[0;36mRayLauncher.launch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m\"\"\"Launches the function on the workers from the driver node.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03mThis function is run on the driver process.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_workers()\n\u001b[0;32m---> 58\u001b[0m ray_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_function_on_workers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRay launcher does not support trainer is None!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/ray_lightning/launchers/ray_launcher.py:249\u001b[0m, in \u001b[0;36mRayLauncher.run_function_on_workers\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_futures \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    241\u001b[0m     w\u001b[38;5;241m.\u001b[39mexecute\u001b[38;5;241m.\u001b[39mremote(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapping_function, i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_global_to_local,\n\u001b[1;32m    242\u001b[0m                      function, model_ref, new_args, kwargs,\n\u001b[1;32m    243\u001b[0m                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtune_queue)\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers)\n\u001b[1;32m    245\u001b[0m ]\n\u001b[1;32m    247\u001b[0m trainer\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 249\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_results\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_futures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtune_queue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/ray_lightning/util.py:64\u001b[0m, in \u001b[0;36mprocess_results\u001b[0;34m(training_result_futures, queue)\u001b[0m\n\u001b[1;32m     62\u001b[0m         _handle_queue(queue)\n\u001b[1;32m     63\u001b[0m     ready, not_ready \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mwait(not_ready, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mready\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m ray\u001b[38;5;241m.\u001b[39mget(ready)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m queue:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Process any remaining items in queue.\u001b[39;00m\n",
      "File \u001b[0;32m/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/ray/_private/worker.py:2280\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2278\u001b[0m     worker\u001b[38;5;241m.\u001b[39mcore_worker\u001b[38;5;241m.\u001b[39mdump_object_store_memory_usage()\n\u001b[1;32m   2279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayTaskError):\n\u001b[0;32m-> 2280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[0;31mRayTaskError(AttributeError)\u001b[0m: \u001b[36mray::RayExecutor.execute()\u001b[39m (pid=1079981, ip=10.100.115.3, repr=<ray_lightning.launchers.utils.RayExecutor object at 0x7fe0a847e700>)\n  File \"/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/ray_lightning/launchers/utils.py\", line 52, in execute\n    return fn(*args, **kwargs)\n  File \"/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/ray_lightning/launchers/ray_launcher.py\", line 301, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 811, in _fit_impl\n    results = self._run(model, ckpt_path=self.ckpt_path)\n  File \"/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1217, in _run\n    self.strategy.setup(self)\n  File \"/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp_spawn.py\", line 141, in setup\n    self.configure_ddp()\n  File \"/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp_spawn.py\", line 193, in configure_ddp\n    self.setup_optimizers(self.lightning_module.trainer)\n  File \"/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py\", line 128, in setup_optimizers\n    self.optimizers, self.lr_scheduler_configs, self.optimizer_frequencies = _init_optimizers_and_lr_schedulers(\n  File \"/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\", line 180, in _init_optimizers_and_lr_schedulers\n    optim_conf = model.trainer._call_lightning_module_hook(\"configure_optimizers\", pl_module=model)\n  File \"/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1595, in _call_lightning_module_hook\n    output = fn(*args, **kwargs)\n  File \"/work/salzubi_umass_edu/T5_human_eval_finetune/finetuner.py\", line 58, in configure_optimizers\n    optimizer = torch.optim.AdamW(self.parameters(), lr=self.h_params.lr)\nAttributeError: 'Namespace' object has no attribute 'lr'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 15:17:13,138\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::RayExecutor.execute()\u001b[39m (pid=1079980, ip=10.100.115.3, repr=<ray_lightning.launchers.utils.RayExecutor object at 0x7efc60481700>)\n",
      "  File \"/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/ray_lightning/launchers/utils.py\", line 52, in execute\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/ray_lightning/launchers/ray_launcher.py\", line 301, in _wrapping_function\n",
      "    results = function(*args, **kwargs)\n",
      "  File \"/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 811, in _fit_impl\n",
      "    results = self._run(model, ckpt_path=self.ckpt_path)\n",
      "  File \"/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1217, in _run\n",
      "    self.strategy.setup(self)\n",
      "  File \"/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp_spawn.py\", line 141, in setup\n",
      "    self.configure_ddp()\n",
      "  File \"/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp_spawn.py\", line 193, in configure_ddp\n",
      "    self.setup_optimizers(self.lightning_module.trainer)\n",
      "  File \"/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py\", line 128, in setup_optimizers\n",
      "    self.optimizers, self.lr_scheduler_configs, self.optimizer_frequencies = _init_optimizers_and_lr_schedulers(\n",
      "  File \"/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\", line 180, in _init_optimizers_and_lr_schedulers\n",
      "    optim_conf = model.trainer._call_lightning_module_hook(\"configure_optimizers\", pl_module=model)\n",
      "  File \"/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1595, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/work/salzubi_umass_edu/T5_human_eval_finetune/finetuner.py\", line 58, in configure_optimizers\n",
      "    optimizer = torch.optim.AdamW(self.parameters(), lr=self.h_params.lr)\n",
      "AttributeError: 'Namespace' object has no attribute 'lr'\n"
     ]
    }
   ],
   "source": [
    "tb_logger = pl_loggers.TensorBoardLogger(save_dir=f\"\")\n",
    "strategy = RayStrategy(num_workers=num_gpus, use_gpu=True if num_gpus > 0 else False, find_unused_parameters=False)\n",
    "es = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=25)\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss', dirpath = \"\", filename = \"DELETE\", mode=\"min\")\n",
    "val_check_interval = args.val_check_interval\n",
    "\n",
    "\n",
    "model = T5FineTuner(args)\n",
    "trainer = pl.Trainer(max_steps = 25000, strategy=strategy, callbacks = [es, checkpoint_callback], val_check_interval=val_check_interval, logger=tb_logger, replace_sampler_ddp=False) # strategy = strategy\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2eb412e-ece1-4812-95af-02b0a562339f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "539\n"
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(train_loader):\n",
    "    pass\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3893bd3-078a-45ae-a722-2d94c14e562a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.conda-wmt_env)",
   "language": "python",
   "name": "conda-env-.conda-wmt_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
