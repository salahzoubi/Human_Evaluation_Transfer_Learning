{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7607cbd-e0ec-4a4b-8f62-d50f02692d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/salzubi_umass_edu/.conda/envs/wmt_env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-06-02 14:10:52.290817: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-02 14:10:52.476117: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-02 14:10:54.086552: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /modules/apps/julia/1.7.2/lib:/modules/apps/cuda/11.3.1/lib64\n",
      "2023-06-02 14:10:54.086674: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /modules/apps/julia/1.7.2/lib:/modules/apps/cuda/11.3.1/lib64\n",
      "2023-06-02 14:10:54.086685: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from data_utils import *\n",
    "from dataset import TLDataset\n",
    "from transformers import T5ForConditionalGeneration,AutoTokenizer, AutoConfig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from finetuner import T5FineTuner\n",
    "import torch \n",
    "from evaluate import *\n",
    "from multiprocessing import cpu_count\n",
    "import argparse\n",
    "output_path_dir = \"/home/salzubi_umass_edu/CKPTS/OAI_TRG_MODEL_CKPT/\"\n",
    "train_bsz = 4\n",
    "lr = 1e-3\n",
    "train_fraction = 1\n",
    "model_name = \"google/flan-t5-base\"\n",
    "file_name = f\"OAI_trg_MODEL_task_Adam_lr:{lr}_bsz:{train_bsz}_frac:{train_fraction}_{model_name}\"\n",
    "num_gpus = torch.cuda.device_count()\n",
    "num_cpus = cpu_count()\n",
    "\n",
    "args_dict = dict(\n",
    "    # file_name = file_name,\n",
    "    # output_dir=output_path_dir, # path to save the checkpoints\n",
    "    model_name_or_path=model_name,\n",
    "    tokenizer_name_or_path=model_name,\n",
    "    max_seq_length=512,\n",
    "    train_fraction= train_fraction,\n",
    "    learning_rate=lr,\n",
    "    weight_decay=0.0,\n",
    "    scheduler_factor = .1,\n",
    "    use_gpu = True if num_gpus > 0 else False,\n",
    "    train_batch_size=train_bsz,\n",
    "    eval_batch_size=32,\n",
    "    num_train_steps=10000000,\n",
    "    es_patience = 4,\n",
    "    val_check_interval = .25,\n",
    "    dropout = .2,\n",
    "    n_gpu=num_gpus,\n",
    "    cpu_per_device=num_cpus,\n",
    "    task = \"classification\",\n",
    ")\n",
    "\n",
    "args = argparse.Namespace(**args_dict)\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c8b0e40-cbbf-4c2f-9b0e-8b7e6cab3f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/salzubi_umass_edu/T5_human_eval_finetune'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494c467c-beb1-47d8-af20-0fc562a4614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import *\n",
    "from sklearn.metrics import accuracy_score, r2_score, mean_squared_error\n",
    "import glob\n",
    "trg_task = [35]\n",
    "test_split = .2\n",
    "fixed_path = \"../experiments/\"\n",
    "# model_path = \"SRC_task_[3]___TRG_task___[8]\"\n",
    "model_path = \"SRC_task_[50, 33, 40, 32, 31, 51, 39, 52, 53, 57, 64, 54, 55, 14]___TRG_task___[35]\"\n",
    "\n",
    "scores = []\n",
    "\n",
    "for i in range(1,4):\n",
    "    random_seed = i\n",
    "    ckpt_path = f\"{fixed_path}{model_path}/trial_{i}/{model_path}___trial_{i}-v1.ckpt\"\n",
    "    # eval_path = f\"{fixed_path}{model_path}/trial_{i}/{model_path}___trial_{i}_eval.csv\"\n",
    "    # pred_df = pd.read_csv(eval_path)\n",
    "    train, val, test = generate_normal_splits(data_path = \"/work/salzubi_umass_edu/human_eval_datasets/master_dataset.csv\", random_state=random_seed,  trg_task_ids= trg_task,  split_size=test_split, source_tuning=False)\n",
    "    test_model = load_ckpt(args, ckpt_path)\n",
    "    test_data = TLDataset(test, tokenizer)\n",
    "    test_loader = DataLoader(test_data, batch_size=args.eval_batch_size, num_workers=num_cpus) #, num_workers=num_cpus//num_gpus\n",
    "    pred_df = evaluate_batch(test_loader, test_model, tokenizer)\n",
    "    scores += [accuracy_score(pred_df.predicted, pred_df.ground_truth)]\n",
    "    # scores += [mean_squared_error(pred_df.predicted, pred_df.ground_truth)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1ae470d-4b6a-47b6-a0f2-ed5eea0f4b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7148936170212766, 0.7319148936170212, 0.7382978723404255]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de56af7f-ad65-46f7-aed4-c163ee58132b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.728368794326241\n",
      "\n",
      "0.009878289558286592\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.mean(scores))\n",
    "print()\n",
    "print(np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a17d029-7ac4-49dd-850c-df55c1a9f3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{fixed_path}{model_path}/trial_{i}/FINAL_EVAL.txt\", \"w\") as file:\n",
    "    file.write(f\"The {i} different accuracies are: {scores}\\n\")\n",
    "    file.write(f\"\\nAVG ACC: {np.mean(scores)}\\n\")\n",
    "    file.write(f\"STD: {np.std(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71346efb-7397-4419-acac-b94bce321c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18f63bf-88fc-46ab-9413-debd7d940f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import *\n",
    "\n",
    "train, val = generate_normal_splits(data_path = \"/work/salzubi_umass_edu/human_eval_datasets/master_dataset.csv\", random_state=1,\n",
    "                                    trg_task_ids=[24,25,26],\n",
    "                                    train_fraction=1, source_tuning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad074ea5-9248-4a9e-a235-6f38259c50d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import *\n",
    "from evaluate import *\n",
    "from dataset import TLDataset\n",
    "from transformers import T5ForConditionalGeneration,AutoTokenizer, AutoConfig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import argparse\n",
    "import logging\n",
    "import torch\n",
    "from multiprocessing import cpu_count\n",
    "from finetuner import T5FineTuner\n",
    "from ray_lightning import RayStrategy\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from tqdm import tqdm\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "import ray\n",
    "import ast\n",
    "import os\n",
    "import gc\n",
    "\n",
    "\n",
    "output_path_dir = \"/home/salzubi_umass_edu/CKPTS/OAI_TRG_MODEL_CKPT/\"\n",
    "train_bsz = 4\n",
    "lr = 1e-3\n",
    "train_fraction = 1\n",
    "model_name = \"google/flan-t5-base\"\n",
    "file_name = f\"OAI_trg_MODEL_task_Adam_lr:{lr}_bsz:{train_bsz}_frac:{train_fraction}_{model_name}\"\n",
    "num_gpus = torch.cuda.device_count()\n",
    "num_cpus = cpu_count()\n",
    "\n",
    "args_dict = dict(\n",
    "    # file_name = file_name,\n",
    "    # output_dir=output_path_dir, # path to save the checkpoints\n",
    "    model_name_or_path=model_name,\n",
    "    tokenizer_name_or_path=model_name,\n",
    "    max_seq_length=512,\n",
    "    train_fraction= train_fraction,\n",
    "    lr=lr,\n",
    "    weight_decay=0.0,\n",
    "    scheduler_factor = .1,\n",
    "    use_gpu = True if num_gpus > 0 else False,\n",
    "    train_batch_size=train_bsz,\n",
    "    eval_batch_size=32,\n",
    "    num_train_steps=10000000,\n",
    "    es_patience = 4,\n",
    "    val_check_interval = .25,\n",
    "    dropout = .2,\n",
    "    n_gpu=num_gpus,\n",
    "    cpu_per_device=num_cpus,\n",
    "    task = \"classification\",\n",
    ")\n",
    "\n",
    "args = argparse.Namespace(**args_dict)\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82605627-56c8-44d9-afa9-cac02dd8c4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import *\n",
    "\n",
    "task_ids = [33,59,21]\n",
    "train, val, _ = generate_normal_splits(data_path = \"/work/salzubi_umass_edu/human_eval_datasets/master_dataset.csv\", random_state=1,\n",
    "                                       trg_task_ids=task_ids, val_split=.3, split_size=.3, train_fraction=1, source_tuning=True, example_threshold=50000)\n",
    "\n",
    "train_data = TLDataset(train, tokenizer)\n",
    "val_data = TLDataset(val, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=8, drop_last=True) # , num_workers=num_cpus//num_gpus, sampler = train_sampler\n",
    "val_loader = DataLoader(val_data, batch_size=8) #, num_workers=num_cpus//num_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dca8fc4e-e339-4604-b405-80909da812a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import *\n",
    "import pandas as pd\n",
    "from dataset import TLDataset\n",
    "\n",
    "train = pd.read_csv(\"../experiments/mocha_tests/processed_mocha_train_k=100.csv\")\n",
    "val = pd.read_csv(\"../experiments/mocha_tests/processed_mocha_val.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e03c35f5-333f-41a3-9795-d94f7cb84399",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TLDataset(train, tokenizer)\n",
    "val_data = TLDataset(val, tokenizer)\n",
    "\n",
    "# train_loader = DataLoader(train_data, batch_size=8, drop_last=True) # , num_workers=num_cpus//num_gpus, sampler = train_sampler\n",
    "val_loader = DataLoader(val_data, batch_size=8) #, num_workers=num_cpus//num_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8adaf4-b5d4-4c98-93ff-6625019c88de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ex in val_loader:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c67c095-999d-4ad0-844c-700a70ab14cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b08e1dcf-bd86-49ec-9906-eeadf0507362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "877\n"
     ]
    }
   ],
   "source": [
    "print(len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a45f16a-e5dd-4667-bf2d-314d5e0db43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "tb_logger = pl_loggers.TensorBoardLogger(save_dir=f\"\")\n",
    "strategy = RayStrategy(num_workers=num_gpus, use_gpu=True if num_gpus > 0 else False, find_unused_parameters=False)\n",
    "es = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=25)\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss', dirpath = \"\", filename = \"DELETE\", mode=\"min\")\n",
    "val_check_interval = args.val_check_interval\n",
    "\n",
    "\n",
    "model = T5FineTuner(args)\n",
    "trainer = pl.Trainer(strategy=strategy, callbacks = [es]) # strategy = strategy\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3893bd3-078a-45ae-a722-2d94c14e562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "df = pd.read_csv(\"../experiments/SRC_task_[50, 33, 40, 32, 31, 51, 39, 52, 53, 57, 64, 54, 55]___TRG_task___None/trial_1/SRC_task_[50, 33, 40, 32, 31, 51, 39, 52, 53, 57, 64, 54, 55]___TRG_task___None___trial_1_eval.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3deab208-7e33-4c23-b588-d84d15b3e512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(df.predicted, df.ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8435b2d9-a94b-4700-9dba-332d9696c692",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-wmt_env]",
   "language": "python",
   "name": "conda-env-.conda-wmt_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
